# -*- coding: utf-8 -*-
"""
YOLOv8æ¨¡å‹è®­ç»ƒè„šæœ¬

è¯¥è„šæœ¬ç”¨äºé…ç½®å’Œå¯åŠ¨YOLOv8æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œæ”¯æŒä¸åŒå¤§å°çš„é¢„è®­ç»ƒæ¨¡å‹é€‰æ‹©å’Œè‡ªå®šä¹‰è®­ç»ƒå‚æ•°ã€‚
è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°æŒ‡å®šç›®å½•ã€‚

æ–°å¢åŠŸèƒ½ï¼š
- è‡ªåŠ¨æ£€æµ‹å¹¶è½¬æ¢LabelMeæ ¼å¼(.json)çš„æ ‡æ³¨æ–‡ä»¶ä¸ºYOLOæ ¼å¼(.txt)
- æ”¯æŒæ‰¹é‡è½¬æ¢trainã€validã€testç›®å½•ä¸‹çš„æ ‡æ³¨æ–‡ä»¶
- ç”¨æˆ·è‡ªå®šä¹‰è®­ç»ƒä»»åŠ¡åç§°
- è‡ªåŠ¨åˆ›å»ºæ—¥å¿—æ–‡ä»¶å¤¹ï¼ŒæŒ‰ä»»åŠ¡åç§°åˆ†ç±»
- è®­ç»ƒå®Œæˆåè¾“å‡ºè¯¦ç»†çš„ç»“æœä¿¡æ¯

ä½¿ç”¨æ–¹æ³•ï¼š
1. ç¡®ä¿å·²å®‰è£…æ‰€éœ€ä¾èµ–åŒ…
2. åœ¨datasetç›®å½•ä¸‹å‡†å¤‡å¥½data.yamlé…ç½®æ–‡ä»¶å’Œè®­ç»ƒæ•°æ®é›†
3. æ•°æ®é›†ç›®å½•ç»“æ„åº”è¯¥æ˜¯ï¼š
   dataset/
   â”œâ”€â”€ train/
   â”‚   â”œâ”€â”€ images/  # è®­ç»ƒå›¾åƒ
   â”‚   â””â”€â”€ labels/  # LabelMeæ ¼å¼çš„.jsonæ ‡æ³¨æ–‡ä»¶æˆ–YOLOæ ¼å¼çš„.txtæ–‡ä»¶
   â”œâ”€â”€ valid/
   â”‚   â”œâ”€â”€ images/  # éªŒè¯å›¾åƒ
   â”‚   â””â”€â”€ labels/  # LabelMeæ ¼å¼çš„.jsonæ ‡æ³¨æ–‡ä»¶æˆ–YOLOæ ¼å¼çš„.txtæ–‡ä»¶
   â””â”€â”€ data.yaml
4. è¿è¡Œæ­¤è„šæœ¬ï¼ŒæŒ‰ç…§æç¤ºé€‰æ‹©æ¨¡å‹å¤§å°å’Œè®¾ç½®è®­ç»ƒç»“æœåç§°
"""
import os
import torch
5

import json
import glob
import logging
from datetime import datetime   
from ultralytics import YOLO

def detailed_dataset_check(dataset_dir):
    """
    è¯¦ç»†æ£€æŸ¥æ•°æ®é›†ï¼ŒéªŒè¯å›¾åƒä¸æ ‡ç­¾çš„å¯¹åº”å…³ç³»å’Œæ ‡ç­¾å†…å®¹
    
    å‚æ•°:
        dataset_dir: æ•°æ®é›†æ ¹ç›®å½•
    """
    import yaml
    
    print('\n=== è¯¦ç»†æ•°æ®é›†æ£€æŸ¥ ===')
    
    # åŠ è½½data.yamlä¸­çš„ç±»åˆ«ä¿¡æ¯
    yaml_path = os.path.join(dataset_dir, 'data.yaml')
    with open(yaml_path, 'r', encoding='utf-8') as f:
        data = yaml.safe_load(f)
    
    expected_classes = data.get('names', [])
    expected_nc = data.get('nc', 0)
    
    print(f'\næœŸæœ›çš„ç±»åˆ«æ•°é‡: {expected_nc}')
    print(f'æœŸæœ›çš„ç±»åˆ«åˆ—è¡¨: {expected_classes}')
    
    # æ£€æŸ¥trainå’Œvalidé›†
    for split in ['train', 'valid']:
        print(f'\n--- {split} é›†è¯¦ç»†æ£€æŸ¥ ---')
        
        # å®šä¹‰è·¯å¾„
        img_dir = os.path.join(dataset_dir, split, 'images')
        lbl_dir = os.path.join(dataset_dir, split, 'labels')
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = glob.glob(os.path.join(img_dir, '*.*'))
        image_files = [f for f in image_files if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
        print(f'å›¾åƒæ–‡ä»¶æ•°é‡: {len(image_files)}')
        
        # è·å–æ‰€æœ‰æ ‡ç­¾æ–‡ä»¶
        label_files = glob.glob(os.path.join(lbl_dir, '*.txt'))
        print(f'æ ‡ç­¾æ–‡ä»¶æ•°é‡: {len(label_files)}')
        
        # è·å–æ‰€æœ‰JSONæ–‡ä»¶
        json_files = glob.glob(os.path.join(lbl_dir, '*.json'))
        print(f'JSONæ ‡æ³¨æ–‡ä»¶æ•°é‡: {len(json_files)}')
        
        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡
        class_counts = {i: 0 for i in range(expected_nc)}
        
        # æ£€æŸ¥æ¯ä¸ªæ ‡ç­¾æ–‡ä»¶
        for lbl_path in label_files[:10]:  # åªæ£€æŸ¥å‰10ä¸ªæ–‡ä»¶
            try:
                with open(lbl_path, 'r') as f:
                    lines = f.readlines()
                
                base_name = os.path.splitext(os.path.basename(lbl_path))[0]
                img_path = None
                
                # æŸ¥æ‰¾å¯¹åº”çš„å›¾åƒæ–‡ä»¶
                for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
                    potential_img = os.path.join(img_dir, f'{base_name}{ext}')
                    if os.path.exists(potential_img):
                        img_path = potential_img
                        break
                
                if img_path:
                    print(f'\næ–‡ä»¶ {base_name}:')
                    print(f'  âœ… å›¾åƒæ–‡ä»¶å­˜åœ¨')
                    print(f'  æ ‡ç­¾è¡Œæ•°: {len(lines)}')
                    
                    # æ£€æŸ¥æ ‡ç­¾å†…å®¹
                    for line_num, line in enumerate(lines[:5], 1):  # åªæ˜¾ç¤ºå‰5è¡Œ
                        line = line.strip()
                        if not line:
                            continue
                        
                        parts = line.split()
                        if len(parts) != 5:
                            print(f'    âŒ ç¬¬{line_num}è¡Œæ ¼å¼é”™è¯¯: {line}')
                            continue
                        
                        cls_id = int(parts[0])
                        x, y, w, h = map(float, parts[1:5])
                        
                        # æ£€æŸ¥ç±»åˆ«IDæ˜¯å¦åœ¨èŒƒå›´å†…
                        if 0 <= cls_id < expected_nc:
                            print(f'    âœ… ç¬¬{line_num}è¡Œ: ç±»åˆ« {cls_id} ({expected_classes[cls_id]}), åæ ‡ {x:.2f},{y:.2f},{w:.2f},{h:.2f}')
                            class_counts[cls_id] += 1
                        else:
                            print(f'    âŒ ç¬¬{line_num}è¡Œ: ç±»åˆ«ID {cls_id} è¶…å‡ºèŒƒå›´ (0-{expected_nc-1})')
                else:
                    print(f'\nâŒ æ–‡ä»¶ {base_name}: ç¼ºå°‘å¯¹åº”çš„å›¾åƒæ–‡ä»¶')
                    
            except Exception as e:
                print(f'\nâŒ æ–‡ä»¶ {os.path.basename(lbl_path)}: è¯»å–é”™è¯¯ - {str(e)}')
        
        # è¾“å‡ºç±»åˆ«ç»Ÿè®¡
        print(f'\nç±»åˆ«ç»Ÿè®¡ (ä»…å‰10ä¸ªæ–‡ä»¶):')
        for cls_id, count in class_counts.items():
            if count > 0:
                print(f'  ç±»åˆ« {cls_id} ({expected_classes[cls_id]}): {count} ä¸ªæ ·æœ¬')
    
    print('\n=== è¯¦ç»†æ£€æŸ¥å®Œæˆ ===')

def validate_dataset(dataset_dir):
    """
    éªŒè¯æ•°æ®é›†çš„å®Œæ•´æ€§ï¼Œæ£€æŸ¥å›¾åƒä¸æ ‡ç­¾æ–‡ä»¶æ˜¯å¦åŒ¹é…ã€æ ‡ç­¾å†…å®¹æ˜¯å¦æ­£ç¡®
    
    å‚æ•°:
        dataset_dir: æ•°æ®é›†æ ¹ç›®å½•
    
    è¿”å›:
        bool: å¦‚æœæ•°æ®é›†éªŒè¯é€šè¿‡è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    import glob
    import yaml
    
    print('\n=== æ•°æ®é›†éªŒè¯ ===')
    has_error = False
    
    # æ£€æŸ¥data.yamlæ–‡ä»¶
    yaml_path = os.path.join(dataset_dir, 'data.yaml')
    if not os.path.exists(yaml_path):
        print(f'âŒ é”™è¯¯: æ‰¾ä¸åˆ°data.yamlæ–‡ä»¶')
        print(f'   è§£å†³æ–¹æ¡ˆ: åœ¨datasetç›®å½•ä¸‹åˆ›å»ºdata.yamlæ–‡ä»¶ï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹:')
        print(f'   train: ../train/images')
        print(f'   val: ../valid/images')
        print(f'   nc: ç±»åˆ«æ•°é‡')
        print(f'   names: ["ç±»åˆ«1", "ç±»åˆ«2", ...]')
        return False
    
    # è¯»å–data.yamlæ–‡ä»¶
    with open(yaml_path, 'r', encoding='utf-8') as f:
        data = yaml.safe_load(f)
    
    expected_nc = data.get('nc', 0)
    
    # æ£€æŸ¥trainå’Œvalidé›†
    for split in ['train', 'valid']:
        print(f'\n--- {split} é›† ---')
        
        # å®šä¹‰è·¯å¾„
        img_dir = os.path.join(dataset_dir, split, 'images')
        lbl_dir = os.path.join(dataset_dir, split, 'labels')
        
        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(img_dir):
            print(f'âŒ é”™è¯¯: æ‰¾ä¸åˆ°{split}/imagesç›®å½•')
            print(f'   è§£å†³æ–¹æ¡ˆ: åˆ›å»º{dataset_dir}/{split}/imagesç›®å½•å¹¶æ”¾å…¥å›¾åƒæ–‡ä»¶')
            has_error = True
            continue
        
        if not os.path.exists(lbl_dir):
            print(f'âŒ é”™è¯¯: æ‰¾ä¸åˆ°{split}/labelsç›®å½•')
            print(f'   è§£å†³æ–¹æ¡ˆ: åˆ›å»º{dataset_dir}/{split}/labelsç›®å½•å¹¶æ”¾å…¥æ ‡ç­¾æ–‡ä»¶')
            has_error = True
            continue
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = glob.glob(os.path.join(img_dir, '*.*'))
        image_files = [f for f in image_files if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
        print(f'å›¾åƒæ–‡ä»¶æ•°é‡: {len(image_files)}')
        
        # è·å–æ‰€æœ‰æ ‡ç­¾æ–‡ä»¶
        label_files = glob.glob(os.path.join(lbl_dir, '*.txt'))
        print(f'æ ‡ç­¾æ–‡ä»¶æ•°é‡: {len(label_files)}')
        
        # è·å–æ‰€æœ‰JSONæ–‡ä»¶
        json_files = glob.glob(os.path.join(lbl_dir, '*.json'))
        if json_files:
            print(f'å‘ç°{len(json_files)}ä¸ªLabelMe JSONæ–‡ä»¶ï¼Œå°†åœ¨è®­ç»ƒå‰è‡ªåŠ¨è½¬æ¢')
        
        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
        total_files = len(image_files) + len(label_files) + len(json_files)
        if split == 'train' and not image_files:
            print(f'âŒ é”™è¯¯: è®­ç»ƒé›†ä¸ºç©ºï¼Œæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å›¾åƒæ–‡ä»¶')
            print(f'   è§£å†³æ–¹æ¡ˆ: å°†å›¾åƒæ–‡ä»¶æ”¾å…¥{dataset_dir}/{split}/imagesç›®å½•')
            has_error = True
        elif split == 'valid' and not image_files:
            print(f'âš ï¸  è­¦å‘Š: éªŒè¯é›†ä¸ºç©ºï¼Œæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å›¾åƒæ–‡ä»¶')
            print(f'   è§£å†³æ–¹æ¡ˆ: è¿è¡Œ python create_validation_set.py ä»è®­ç»ƒé›†åˆ†å‰²ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯é›†')
            has_error = True
        
        # æ£€æŸ¥æ¯ä¸ªå›¾åƒæ˜¯å¦æœ‰å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶
        missing_labels = []
        invalid_labels = []
        
        for img_path in image_files:
            # è·å–å›¾åƒæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            base_name = os.path.splitext(os.path.basename(img_path))[0]
            # å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶è·¯å¾„
            lbl_path = os.path.join(lbl_dir, f'{base_name}.txt')
            json_path = os.path.join(lbl_dir, f'{base_name}.json')
            
            if not os.path.exists(lbl_path) and not os.path.exists(json_path):
                missing_labels.append(base_name)
                continue
            
            # å¦‚æœå­˜åœ¨JSONæ–‡ä»¶ä½†ä¸å­˜åœ¨TXTæ–‡ä»¶ï¼Œä¼šåœ¨è®­ç»ƒå‰è‡ªåŠ¨è½¬æ¢
            if not os.path.exists(lbl_path) and os.path.exists(json_path):
                continue
            
            # æ£€æŸ¥æ ‡ç­¾æ–‡ä»¶å†…å®¹
            try:
                with open(lbl_path, 'r') as f:
                    lines = f.readlines()
                
                if not lines:
                    invalid_labels.append((base_name, 'æ ‡ç­¾æ–‡ä»¶ä¸ºç©º'))
                    continue
                
                for line_num, line in enumerate(lines, 1):
                    line = line.strip()
                    if not line:
                        continue
                    
                    # æ£€æŸ¥æ ‡ç­¾æ ¼å¼
                    parts = line.split()
                    if len(parts) != 5:
                        invalid_labels.append((base_name, f'ç¬¬{line_num}è¡Œæ ¼å¼é”™è¯¯: {line}'))
                        continue
                    
                    # æ£€æŸ¥ç±»åˆ«IDå’Œåæ ‡
                    try:
                        cls_id = int(parts[0])
                        x, y, w, h = map(float, parts[1:5])
                    except ValueError:
                        invalid_labels.append((base_name, f'ç¬¬{line_num}è¡Œæ•°å€¼æ ¼å¼é”™è¯¯: {line}'))
                        continue
                    
                    if cls_id < 0 or (expected_nc > 0 and cls_id >= expected_nc):
                        invalid_labels.append((base_name, f'ç¬¬{line_num}è¡Œç±»åˆ«IDè¶…å‡ºèŒƒå›´: {cls_id}'))
                        
                    if not (0 <= x <= 1 and 0 <= y <= 1 and 0 <= w <= 1 and 0 <= h <= 1):
                        invalid_labels.append((base_name, f'ç¬¬{line_num}è¡Œåæ ‡è¶…å‡ºèŒƒå›´: {x:.2f},{y:.2f},{w:.2f},{h:.2f}'))
                        
            except Exception as e:
                invalid_labels.append((base_name, f'è¯»å–é”™è¯¯: {str(e)}'))
        
        # è¾“å‡ºç»“æœ
        if missing_labels:
            print(f'âŒ ç¼ºå¤±æ ‡ç­¾æ–‡ä»¶: {len(missing_labels)}')
            for name in missing_labels[:5]:
                print(f'  - {name}.jpg')
            print(f'   è§£å†³æ–¹æ¡ˆ: ä¸ºä¸Šè¿°å›¾åƒåˆ›å»ºå¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶ï¼Œæˆ–è€…ä½¿ç”¨python convert_labelme_to_yolo.pyè½¬æ¢JSONæ ‡æ³¨')
            has_error = True
        
        if invalid_labels:
            print(f'âŒ æ— æ•ˆæ ‡ç­¾æ–‡ä»¶: {len(invalid_labels)}')
            for name, err in invalid_labels[:5]:
                print(f'  - {name}.txt: {err}')
            print(f'   è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥å¹¶ä¿®å¤ä¸Šè¿°æ ‡ç­¾æ–‡ä»¶çš„æ ¼å¼å’Œå†…å®¹ï¼Œæˆ–ä½¿ç”¨ python fix_dataset.py å°è¯•è‡ªåŠ¨ä¿®å¤')
            has_error = True
    
    if has_error:
        print('\n=== éªŒè¯å¤±è´¥ï¼Œè¯·æ ¹æ®ä¸Šè¿°é”™è¯¯ä¿¡æ¯è¿›è¡Œä¿®å¤ ===')
        return False
    else:
        print('\n=== éªŒè¯æˆåŠŸï¼Œæ•°æ®é›†å®Œæ•´æ€§è‰¯å¥½ ===')
        return True

def convert_labelme_to_yolo(json_file, class_mapping=None):
    """
    å°†å•ä¸ªLabelMe JSONæ ¼å¼çš„æ³¨é‡Šè½¬æ¢ä¸ºYOLOæ ¼å¼çš„æ–‡æœ¬æ–‡ä»¶
    
    å‚æ•°:
        json_file: LabelMeæ ¼å¼çš„JSONæ³¨é‡Šæ–‡ä»¶è·¯å¾„
        class_mapping: ç±»åˆ«åç§°åˆ°IDçš„æ˜ å°„å­—å…¸
    """
    # åŠ è½½LabelMe JSONæ–‡ä»¶
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æå–å›¾åƒä¿¡æ¯
    image_width = data.get('imageWidth')
    image_height = data.get('imageHeight')
    
    if not image_width or not image_height:
        print(f"è­¦å‘Š: {json_file} ä¸­ç¼ºå°‘å›¾åƒå°ºå¯¸ä¿¡æ¯ï¼Œè·³è¿‡è½¬æ¢")
        return False, {}
    
    # æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆä¸JSONæ–‡ä»¶åŒåï¼Œæ‰©å±•åä¸º.txtï¼‰
    base_name = os.path.splitext(json_file)[0]
    output_file = f"{base_name}.txt"
    
    # å¦‚æœæ²¡æœ‰æä¾›ç±»åˆ«æ˜ å°„ï¼Œè‡ªåŠ¨ä»æ•°æ®ä¸­æå–
    if class_mapping is None:
        class_mapping = {}
        for shape in data.get('shapes', []):
            label = shape['label']
            if label not in class_mapping:
                class_mapping[label] = len(class_mapping)
    
    # å¤„ç†æ¯ä¸ªæ ‡æ³¨å½¢çŠ¶
    with open(output_file, 'w') as f:
        for shape in data.get('shapes', []):
            label = shape['label']
            points = shape['points']
            shape_type = shape.get('shape_type', 'rectangle')
            
            # å°†æ ‡ç­¾åç§°è½¬æ¢ä¸ºå°å†™è¿›è¡ŒåŒ¹é…
            label_lower = label.lower()
            
            # è·å–ç±»åˆ«ID
            if label_lower not in class_mapping:
                # å¦‚æœç±»åˆ«ä¸åœ¨æ˜ å°„ä¸­ï¼Œè·³è¿‡è¯¥æ ‡æ³¨
                print(f"è­¦å‘Š: {json_file} ä¸­åŒ…å«æœªåœ¨data.yamlä¸­å®šä¹‰çš„ç±»åˆ« {label}ï¼Œè·³è¿‡æ­¤æ ‡æ³¨")
                continue
            else:
                class_id = class_mapping[label_lower]
            
            # æ ¹æ®å½¢çŠ¶ç±»å‹è®¡ç®—è¾¹ç•Œæ¡†
            if shape_type == 'rectangle':
                # çŸ©å½¢ï¼šLabelMeå­˜å‚¨å››ä¸ªè§’ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦æå–æ‰€æœ‰ç‚¹çš„åæ ‡èŒƒå›´
                x_coords = [p[0] for p in points]
                y_coords = [p[1] for p in points]
                x1, y1 = min(x_coords), min(y_coords)
                x2, y2 = max(x_coords), max(y_coords)
            elif shape_type == 'polygon':
                # å¤šè¾¹å½¢ï¼šè½¬æ¢ä¸ºè¾¹ç•Œæ¡†
                x_coords = [p[0] for p in points]
                y_coords = [p[1] for p in points]
                x1, y1 = min(x_coords), min(y_coords)
                x2, y2 = max(x_coords), max(y_coords)
            else:
                print(f"è­¦å‘Š: {json_file} ä¸­åŒ…å«ä¸æ”¯æŒçš„å½¢çŠ¶ç±»å‹ {shape_type}ï¼Œè·³è¿‡æ­¤æ ‡æ³¨")
                continue
            
            # è®¡ç®—YOLOæ ¼å¼çš„å½’ä¸€åŒ–åæ ‡
            # YOLOæ ¼å¼ï¼š[x_center, y_center, width, height]
            x_center = (x1 + x2) / 2 / image_width
            y_center = (y1 + y2) / 2 / image_height
            norm_width = abs(x2 - x1) / image_width
            norm_height = abs(y2 - y1) / image_height
            
            # å†™å…¥YOLOæ ¼å¼çš„è¡Œ
            line = f"{class_id} {x_center} {y_center} {norm_width} {norm_height}\n"
            f.write(line)
    
    return True, class_mapping

def load_class_mapping_from_yaml(yaml_path):
    """
    ä»data.yamlæ–‡ä»¶åŠ è½½ç±»åˆ«æ˜ å°„
    
    å‚æ•°:
        yaml_path: data.yamlæ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        class_mapping: ç±»åˆ«åç§°åˆ°IDçš„æ˜ å°„å­—å…¸ï¼ˆå°å†™é”®ï¼‰
    """
    import yaml
    with open(yaml_path, 'r', encoding='utf-8') as f:
        data = yaml.safe_load(f)
    
    class_mapping = {}
    if 'names' in data:
        for idx, name in enumerate(data['names']):
            # å°†æ‰€æœ‰ç±»åˆ«åç§°è½¬æ¢ä¸ºå°å†™ï¼Œç¡®ä¿å¤§å°å†™ä¸æ•æ„ŸåŒ¹é…
            class_mapping[name.lower()] = idx
    
    return class_mapping

def batch_convert_labelme_to_yolo(dataset_dir):
    """
    æ‰¹é‡è½¬æ¢æ•°æ®é›†ç›®å½•ä¸‹çš„æ‰€æœ‰LabelMe JSONæ–‡ä»¶ä¸ºYOLOæ ¼å¼
    
    å‚æ•°:
        dataset_dir: æ•°æ®é›†æ ¹ç›®å½•
    """
    # éœ€è¦æ£€æŸ¥çš„å­ç›®å½•
    subdirs = ['train/labels', 'valid/labels', 'test/labels']
    
    # ä»data.yamlåŠ è½½ç±»åˆ«æ˜ å°„
    yaml_path = os.path.join(dataset_dir, 'data.yaml')
    class_mapping = load_class_mapping_from_yaml(yaml_path)
    
    print("å¼€å§‹æ£€æŸ¥å¹¶è½¬æ¢LabelMeæ ¼å¼æ ‡æ³¨æ–‡ä»¶...")
    print(f"ä½¿ç”¨data.yamlä¸­çš„ç±»åˆ«æ˜ å°„: {class_mapping}")
    
    for subdir in subdirs:
        # æ„å»ºå®Œæ•´è·¯å¾„
        labels_dir = os.path.join(dataset_dir, subdir)
        
        if not os.path.exists(labels_dir):
            print(f"ç›®å½• {labels_dir} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            continue
        
        # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
        json_files = glob.glob(os.path.join(labels_dir, '*.json'))
        
        if not json_files:
            print(f"åœ¨ {labels_dir} ä¸­æœªæ‰¾åˆ°LabelMe JSONæ–‡ä»¶")
            continue
        
        print(f"åœ¨ {labels_dir} ä¸­æ‰¾åˆ° {len(json_files)} ä¸ªLabelMe JSONæ–‡ä»¶ï¼Œå¼€å§‹è½¬æ¢...")
        
        success_count = 0
        for json_file in json_files:
            # ä½¿ç”¨data.yamlä¸­çš„ç±»åˆ«æ˜ å°„ï¼Œè€Œä¸æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„
            result, _ = convert_labelme_to_yolo(json_file, class_mapping)
            if result:
                success_count += 1
        
        print(f"  è½¬æ¢å®Œæˆï¼æˆåŠŸè½¬æ¢ {success_count} ä¸ªæ–‡ä»¶ï¼Œå¤±è´¥ {len(json_files) - success_count} ä¸ªæ–‡ä»¶")
    
    # æ‰“å°ä½¿ç”¨çš„ç±»åˆ«æ˜ å°„
    if class_mapping:
        print("\nä½¿ç”¨çš„ç±»åˆ«æ˜ å°„:")
        for label, idx in sorted(class_mapping.items(), key=lambda x: x[1]):
            print(f"  {label}: {idx}")
    
    return class_mapping

def setup_logging(log_dir):
    """
    è®¾ç½®æ—¥å¿—è®°å½•å™¨
    
    å‚æ•°:
        log_dir: æ—¥å¿—ä¿å­˜ç›®å½•
    """
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs(log_dir, exist_ok=True)
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶è·¯å¾„
    log_file = os.path.join(log_dir, f"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    
    # é…ç½®æ—¥å¿—è®°å½•å™¨
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),  # æ˜ç¡®æŒ‡å®šUTF-8ç¼–ç 
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger(__name__)

def main():
    """
    ä¸»å‡½æ•°ï¼Œè´Ÿè´£YOLOv8æ¨¡å‹è®­ç»ƒçš„æ•´ä¸ªæµç¨‹
    
    æµç¨‹åŒ…æ‹¬ï¼š
    1. æ£€æŸ¥å¹¶é€‰æ‹©è®­ç»ƒè®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
    2. è‡ªåŠ¨æ£€æµ‹å¹¶è½¬æ¢LabelMeæ ¼å¼æ ‡æ³¨æ–‡ä»¶
    3. éªŒè¯æ•°æ®é›†å®Œæ•´æ€§
    4. è¯¦ç»†æ£€æŸ¥æ•°æ®é›†
    5. è®©ç”¨æˆ·é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹å¤§å°
    6. è®¾ç½®è®­ç»ƒç»“æœåç§°
    7. åˆ›å»ºæ—¥å¿—æ–‡ä»¶å¤¹
    8. é…ç½®è®­ç»ƒå‚æ•°
    9. è¿è¡Œæ¨¡å‹è®­ç»ƒ
    10. è¯„ä¼°è®­ç»ƒåçš„æ¨¡å‹
    11. è¾“å‡ºè®­ç»ƒç»“æœä¿¡æ¯
    """
    # æ£€æŸ¥CUDAï¼ˆGPUæ”¯æŒï¼‰æ˜¯å¦å¯ç”¨
    training_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    print("Using device:", training_device)

    # è·å–æ•°æ®é›†æ ¹ç›®å½•
    dataset_dir = os.path.abspath('dataset')
    
    # æ£€æŸ¥æ•°æ®é›†ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(dataset_dir):
        print(f'âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®é›†ç›®å½• {dataset_dir}')
        print(f'   è§£å†³æ–¹æ¡ˆ: åˆ›å»ºdatasetç›®å½•å¹¶æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç»„ç»‡æ•°æ®:')
        print(f'   dataset/')
        print(f'   â”œâ”€â”€ data.yaml')
        print(f'   â”œâ”€â”€ train/')
        print(f'   â”‚   â”œâ”€â”€ images/')
        print(f'   â”‚   â””â”€â”€ labels/')
        print(f'   â””â”€â”€ valid/')
        print(f'       â”œâ”€â”€ images/')
        print(f'       â””â”€â”€ labels/')
        input("\nPress Enter to exit...")
        return
    
    # è‡ªåŠ¨è½¬æ¢LabelMeæ ¼å¼æ ‡æ³¨æ–‡ä»¶
    print("\n=== å¼€å§‹è‡ªåŠ¨è½¬æ¢LabelMeæ ‡æ³¨æ–‡ä»¶ ===")
    class_mapping = batch_convert_labelme_to_yolo(dataset_dir)
    if not class_mapping:
        print(f'âŒ è­¦å‘Š: æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç±»åˆ«æ˜ å°„')
        print(f'   è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥datasetç›®å½•ä¸‹çš„data.yamlæ–‡ä»¶æ˜¯å¦åŒ…å«æ­£ç¡®çš„ç±»åˆ«ä¿¡æ¯')
    
    # éªŒè¯æ•°æ®é›†å®Œæ•´æ€§
    print("\n=== å¼€å§‹éªŒè¯æ•°æ®é›†å®Œæ•´æ€§ ===")
    if not validate_dataset(dataset_dir):
        print(f'\nâŒ è®­ç»ƒæ— æ³•ç»§ç»­ï¼Œæ•°æ®é›†éªŒè¯å¤±è´¥')
        print(f'   è¯·æ ¹æ®ä¸Šè¿°é”™è¯¯ä¿¡æ¯ä¿®å¤æ•°æ®é›†é—®é¢˜åé‡æ–°è¿è¡Œ')
        input("\nPress Enter to exit...")
        return
    
    # è¯¦ç»†æ£€æŸ¥æ•°æ®é›†
    print("\n=== å¼€å§‹è¯¦ç»†æ£€æŸ¥æ•°æ®é›† ===")
    detailed_dataset_check(dataset_dir)

    # æ¨¡å‹é€‰é¡¹å­—å…¸ï¼ŒåŒ…å«ä¸åŒå¤§å°çš„YOLOv8é¢„è®­ç»ƒæ¨¡å‹
    # ä»å¤§åˆ°å°æ’åˆ—ï¼šExtra Large -> Nano
    model_options = {
        "1": "yolov8x.pt",  # Extra Largeï¼Œæœ€å¤§çš„æ¨¡å‹ï¼Œæ€§èƒ½æœ€å¥½ä½†è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦æœ€æ…¢
        "2": "yolov8l.pt",  # Large
        "3": "yolov8m.pt",  # Medium
        "4": "yolov8s.pt",  # Small
        "5": "yolov8n.pt"   # Nanoï¼Œæœ€å°çš„æ¨¡å‹ï¼Œè®­ç»ƒå’Œæ¨ç†é€Ÿåº¦æœ€å¿«ä½†æ€§èƒ½å¯èƒ½è¾ƒä½
    }

    # æ˜¾ç¤ºæ¨¡å‹é€‰æ‹©èœå•
    print("\n=== é€‰æ‹©YOLOv8æ¨¡å‹ ===")
    for key, value in model_options.items():
        # è·å–æ¨¡å‹æè¿°æ³¨é‡Š
        comment_start = model_options[key].find("#")
        if comment_start != -1:
            model_desc = model_options[key][comment_start + 2:]
        else:
            model_desc = ""
        print(f"   {key}. {value.split()[0]} - {model_desc}")
    
    # è·å–ç”¨æˆ·é€‰æ‹©
    while True:
        choice = input("\nè¯·è¾“å…¥æ¨¡å‹ç¼–å· (1-5): ").strip()
        if choice in model_options:
            break
        print(f"âŒ è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥1-5ä¹‹é—´çš„æ•°å­—")
    
    selected_model_name = model_options[choice]
    
    # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²å­˜åœ¨è¯¥æ¨¡å‹æ–‡ä»¶
    local_model_path = os.path.abspath(selected_model_name)
    if os.path.exists(local_model_path):
        starting_model = local_model_path
        print(f"ä½¿ç”¨æœ¬åœ°å·²å­˜åœ¨çš„æ¨¡å‹: {starting_model}")
    else:
        # è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œä¸è¯¢é—®ç”¨æˆ·
        print(f"æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹æ–‡ä»¶ {selected_model_name}ï¼Œå°†è‡ªåŠ¨ä»UltralyticsæœåŠ¡å™¨ä¸‹è½½...")
        starting_model = selected_model_name
        print(f"æ¨¡å‹å°†è‡ªåŠ¨ä¸‹è½½å¹¶ä½¿ç”¨: {starting_model}")

    # è·å–ç”¨æˆ·è¾“å…¥çš„è®­ç»ƒä»»åŠ¡åç§°
    import datetime
    default_name = f"watermark_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
    custom_name = input(f"\nè¯·è¾“å…¥è®­ç»ƒä»»åŠ¡åç§° (é»˜è®¤: {default_name}): ").strip()
    if not custom_name:
        custom_name = default_name
    
    print(f"\nè®­ç»ƒç»“æœå°†ä¿å­˜åˆ°: training_output/{custom_name}")
    print(f"æ—¥å¿—æ–‡ä»¶å°†ä¿å­˜åˆ°: log/{custom_name}")

    # è®­ç»ƒå‚æ•°è®¾ç½®
    output_dir = 'training_output'      # è®­ç»ƒç»“æœè¾“å‡ºç›®å½•
    log_base_dir = 'log'                # æ—¥å¿—æ ¹ç›®å½•
    log_dir = os.path.join(log_base_dir, custom_name)  # ä»»åŠ¡æ—¥å¿—ç›®å½•
    batch_size = 8                      # æ‰¹å¤„ç†å¤§å°ï¼Œä¸ä¹‹å‰æˆåŠŸè®­ç»ƒä¸€è‡´
    epoch_count = 50                   # è®­ç»ƒè½®æ•°ï¼Œæ¢å¤åˆ°ä¹‹å‰çš„150è½®
    img_size = 640                      # è¾“å…¥å›¾åƒå¤§å°
    lr0 = 0.0001                        # åˆå§‹å­¦ä¹ ç‡ï¼Œä½¿ç”¨æ›´å°çš„å€¼ä»¥æé«˜ç¨³å®šæ€§
    weight_decay = 0.0005               # æƒé‡è¡°å‡
    momentum = 0.937                    # åŠ¨é‡
    cos_lr = True                       # ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦

    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # è®¾ç½®æ—¥å¿—è®°å½•
    logger = setup_logging(log_dir)
    logger.info("YOLOv8è®­ç»ƒä»»åŠ¡å¼€å§‹")
    logger.info(f"è®­ç»ƒä»»åŠ¡åç§°: {custom_name}")
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {training_device}")
    logger.info(f"é€‰æ‹©æ¨¡å‹: {starting_model}")

    # è·å–æ•°æ®é›†é…ç½®æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    dataset_path = os.path.abspath('dataset/data.yaml')
    
    # åˆ é™¤æ—§çš„æ ‡ç­¾ç¼“å­˜æ–‡ä»¶ï¼Œå¼ºåˆ¶YOLOé‡æ–°è§£ææ ‡ç­¾
    cache_files = [
        os.path.join(os.path.dirname(dataset_path), 'train', 'labels.cache'),
        os.path.join(os.path.dirname(dataset_path), 'valid', 'labels.cache'),
        os.path.join(os.path.dirname(dataset_path), 'test', 'labels.cache')
    ]
    for cache_file in cache_files:
        if os.path.exists(cache_file):
            os.remove(cache_file)
            logger.info(f"å·²åˆ é™¤æ—§çš„æ ‡ç­¾ç¼“å­˜æ–‡ä»¶: {cache_file}")
    
    # æ£€æŸ¥å¹¶åˆ é™¤training_outputç›®å½•ä¸‹çš„æ‰€æœ‰.cacheæ–‡ä»¶
    training_output_dir = os.path.join(os.getcwd(), 'training_output')
    if os.path.exists(training_output_dir):
        for root, dirs, files in os.walk(training_output_dir):
            for file in files:
                if file.endswith('.cache'):
                    cache_file = os.path.join(root, file)
                    try:
                        os.remove(cache_file)
                        logger.info(f"å·²åˆ é™¤è®­ç»ƒè¾“å‡ºç›®å½•ä¸‹çš„ç¼“å­˜æ–‡ä»¶: {cache_file}")
                    except Exception as e:
                        logger.error(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥ {cache_file}: {str(e)}")
    
    # éªŒè¯ç¼“å­˜æ˜¯å¦å·²åˆ é™¤
    logger.info("éªŒè¯ç¼“å­˜æ–‡ä»¶åˆ é™¤æƒ…å†µ:")
    for cache_file in cache_files:
        if os.path.exists(cache_file):
            logger.warning(f"è­¦å‘Š: ç¼“å­˜æ–‡ä»¶ä»å­˜åœ¨ {cache_file}")
        else:
            logger.info(f"ç¡®è®¤: ç¼“å­˜æ–‡ä»¶å·²åˆ é™¤ {cache_file}")
    
    # åŠ è½½YOLOæ¨¡å‹
    print(f"æ­£åœ¨åŠ è½½YOLOæ¨¡å‹: {starting_model}")
    logger.info(f"æ­£åœ¨åŠ è½½YOLOæ¨¡å‹: {starting_model}")
    
    try:
        modelYolo = YOLO(starting_model)
        print("YOLOæ¨¡å‹åŠ è½½å®Œæˆ")
        logger.info("YOLOæ¨¡å‹åŠ è½½å®Œæˆ")
        
        # è°ƒè¯•è¾“å‡ºï¼šæ‰“å°æ¨¡å‹é…ç½®å’Œå‚æ•°
        logger.info(f"æ¨¡å‹ç±»å‹: {type(modelYolo.model)}")
        logger.info(f"æ¨¡å‹è®¾å¤‡: {next(modelYolo.model.parameters()).device}")
        logger.info(f"æ¨¡å‹ç±»åˆ«æ•°: {modelYolo.model.model[-1].nc}")
        
        # æ‰“å°å‰å‡ å±‚çš„å‚æ•°ä¿¡æ¯
        for name, param in list(modelYolo.model.named_parameters())[:5]:
            logger.info(f"å‚æ•° {name}: å½¢çŠ¶={param.shape}, å‡å€¼={param.mean().item():.4f}, æ ‡å‡†å·®={param.std().item():.4f}")
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        input("\nPress Enter to exit...")
        return
    
    try:
        # æ£€æŸ¥æ•°æ®é›†é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(dataset_path):
            print(f'âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®é›†é…ç½®æ–‡ä»¶ {dataset_path}')
            print(f'   è§£å†³æ–¹æ¡ˆ: ç¡®ä¿data.yamlæ–‡ä»¶å­˜åœ¨äºdatasetç›®å½•ä¸­')
            input("\nPress Enter to exit...")
            return
        
        # æ·»åŠ æ›´å¤šè°ƒè¯•ä¿¡æ¯
        logger.info(f"æ•°æ®é›†è·¯å¾„: {dataset_path}")
        logger.info(f"ç±»åˆ«æ˜ å°„: {class_mapping}")
        logger.info(f"è®­ç»ƒå‚æ•°: epochs={epoch_count}, batch={batch_size}, imgsz={img_size}, lr0={lr0}")
        
        # å¼€å§‹è®­ç»ƒæ¨¡å‹
        logger.info("å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        try:
            modelYolo.train(
                data=dataset_path,              # æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
                epochs=epoch_count,             # è®­ç»ƒè½®æ•°
                batch=batch_size,               # æ‰¹å¤„ç†å¤§å°
                imgsz=img_size,                 # è¾“å…¥å›¾åƒå¤§å°
                device=training_device,         # è®­ç»ƒè®¾å¤‡
                project=output_dir,             # è¾“å‡ºé¡¹ç›®ç›®å½•
                name=custom_name,               # è®­ç»ƒç»“æœåç§°
                workers=2,                      # å·¥ä½œçº¿ç¨‹æ•°ï¼Œä¸ºWindowsç¨³å®šæ€§é™ä½æ­¤å€¼
                lr0=lr0,                        # åˆå§‹å­¦ä¹ ç‡
                weight_decay=weight_decay,      # æƒé‡è¡°å‡
                momentum=momentum,              # åŠ¨é‡
                cos_lr=cos_lr,                  # ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
                cache=False,                    # ç¦ç”¨ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°è§£ææ ‡ç­¾
                augment=True,                   # å¯ç”¨æ•°æ®å¢å¼ºï¼Œä¸ä¹‹å‰æˆåŠŸè®­ç»ƒä¸€è‡´
                mosaic=1.0,                     # å¯ç”¨é©¬èµ›å…‹æ•°æ®å¢å¼º
                fliplr=0.5,                     # å¯ç”¨æ°´å¹³ç¿»è½¬
                flipud=0.0,                     # ç¦ç”¨å‚ç›´ç¿»è½¬
                mixup=0.0,                      # ç¦ç”¨æ··åˆ
                amp=False                       # ç¦ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼Œæé«˜æ•°å€¼ç¨³å®šæ€§
            )
        except Exception as train_error:
            logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(train_error)}")
            print("\n" + "="*60)
            print("æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼")
            print("="*60)
            print(f"é”™è¯¯ä¿¡æ¯: {str(train_error)}")
            
            # æä¾›å¸¸è§é”™è¯¯çš„è§£å†³æ–¹æ¡ˆ
            if "CUDA out of memory" in str(train_error):
                print(f"\nğŸ” é—®é¢˜åˆ†æ: GPUå†…å­˜ä¸è¶³")
                print(f"   è§£å†³æ–¹æ¡ˆ: å‡å°batch_sizeï¼ˆå½“å‰ä¸º{batch_size}ï¼‰æˆ–imgszï¼ˆå½“å‰ä¸º{img_size}ï¼‰")
                print(f"   ä¾‹å¦‚: å°†batch_sizeæ”¹ä¸º4ï¼Œimgszæ”¹ä¸º480")
            elif "No labels found" in str(train_error):
                print(f"\nğŸ” é—®é¢˜åˆ†æ: æœªæ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶")
                print(f"   è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥labelsç›®å½•ä¸­çš„æ ‡ç­¾æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")
                print(f"   æ ‡ç­¾æ–‡ä»¶åº”ä¸º.txtæ ¼å¼ï¼Œæ¯è¡ŒåŒ…å«ï¼šç±»åˆ«ID ä¸­å¿ƒç‚¹x ä¸­å¿ƒç‚¹y å®½åº¦ é«˜åº¦")
            elif "AssertionError" in str(train_error) and "dataset" in str(train_error):
                print(f"\nğŸ” é—®é¢˜åˆ†æ: æ•°æ®é›†é…ç½®é”™è¯¯")
                print(f"   è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥data.yamlæ–‡ä»¶ä¸­çš„è·¯å¾„å’Œç±»åˆ«é…ç½®æ˜¯å¦æ­£ç¡®")
            else:
                print(f"\nğŸ” å»ºè®®: æŸ¥çœ‹å®Œæ•´æ—¥å¿—æ–‡ä»¶ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯")
            
            print(f"æ—¥å¿—ç›®å½•: {os.path.abspath(log_dir)}")
            print("="*60)
            input("\nPress Enter to exit...")
            return
        
        logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½
        logger.info("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        try:
            metrics = modelYolo.val()
        except Exception as val_error:
            logger.error(f"æ¨¡å‹è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(val_error)}")
            print("\nâš ï¸  è­¦å‘Š: æ¨¡å‹è¯„ä¼°å¤±è´¥ï¼Œä½†è®­ç»ƒå·²å®Œæˆ")
            print(f"é”™è¯¯ä¿¡æ¯: {str(val_error)}")
        
        logger.info("æ¨¡å‹è¯„ä¼°å®Œæˆ")
        
        # è®­ç»ƒæˆåŠŸä¿¡æ¯
        training_success = True
        
        # å°†é»˜è®¤çš„best.pté‡å‘½åä¸ºç”¨æˆ·è¾“å…¥çš„åç§°
        default_model_path = os.path.join(output_dir, custom_name, 'weights', 'best.pt')
        custom_model_path = os.path.join(output_dir, custom_name, 'weights', f'{custom_name}.pt')
        
        if os.path.exists(default_model_path):
            os.rename(default_model_path, custom_model_path)
            logger.info(f"æ¨¡å‹æ–‡ä»¶å·²ä»best.pté‡å‘½åä¸º{custom_name}.pt")
        
        # å°†æ¨¡å‹æ–‡ä»¶å¤åˆ¶åˆ°modelsæ–‡ä»¶å¤¹ï¼Œæ–¹ä¾¿ç”¨æˆ·æŸ¥æ‰¾
        models_folder = os.path.join(os.getcwd(), 'models')
        os.makedirs(models_folder, exist_ok=True)  # ç¡®ä¿modelsæ–‡ä»¶å¤¹å­˜åœ¨
        
        models_model_path = os.path.join(models_folder, f'{custom_name}.pt')
        
        # ä½¿ç”¨shutil.copy2ä¿ç•™æ–‡ä»¶å…ƒæ•°æ®
        import shutil
        shutil.copy2(custom_model_path, models_model_path)
        logger.info(f"æ¨¡å‹æ–‡ä»¶å·²å¤åˆ¶åˆ°modelsæ–‡ä»¶å¤¹: {models_model_path}")
        
        model_path = models_model_path
        
        # è¾“å‡ºè®­ç»ƒç»“æœä¿¡æ¯
        print("\n" + "="*60)
        print("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print("="*60)
        print(f"è®­ç»ƒçŠ¶æ€: {'æˆåŠŸ' if training_success else 'å¤±è´¥'}")
        print(f"è®­ç»ƒä»»åŠ¡åç§°: {custom_name}")
        print(f"æ¨¡å‹è·¯å¾„: {os.path.abspath(model_path)}")
        print(f"æ—¥å¿—ç›®å½•: {os.path.abspath(log_dir)}")
        print(f"è®­ç»ƒè¾“å‡ºç›®å½•: {os.path.abspath(os.path.join(output_dir, custom_name))}")
        print("="*60)
        
        # è®°å½•åˆ°æ—¥å¿—
        logger.info(f"è®­ç»ƒç»“æœ: {'æˆåŠŸ' if training_success else 'å¤±è´¥'}")
        logger.info(f"æœ€ä½³æ¨¡å‹è·¯å¾„: {os.path.abspath(model_path)}")
        if logger.handlers:
            logger.info(f"æ—¥å¿—æ–‡ä»¶è·¯å¾„: {logger.handlers[0].baseFilename}")
    except Exception as e:
        training_success = False
        logger.error(f"ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        print("\n" + "="*60)
        print("ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼")
        print("="*60)
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"æ—¥å¿—ç›®å½•: {os.path.abspath(log_dir)}")
        print(f"\nğŸ” å»ºè®®: æŸ¥çœ‹å®Œæ•´æ—¥å¿—æ–‡ä»¶ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯")
        print("="*60)

    # ç­‰å¾…ç”¨æˆ·è¾“å…¥åé€€å‡º
    input("\nPress Enter to exit...")

# ç¡®ä¿è„šæœ¬åœ¨ç›´æ¥è¿è¡Œæ—¶æ‰§è¡Œmainå‡½æ•°
if __name__ == '__main__':
    main()
